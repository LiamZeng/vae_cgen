{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2 \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Input, BatchNormalization, ReLU, AveragePooling2D\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dropout, UpSampling2D\nfrom tensorflow.keras.layers import Lambda, Subtract, Add\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.optimizers import RMSprop, Adam, SGD\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# clf = load_model('../input/glass-classifier/glass_cf_15_sigmoid.tf')\nclf = load_model('../input/smileclf/smiling_cf_15_sigmoid.tf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_dim = 400","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a sampling layer\nfrom tensorflow.keras.layers import Layer\nclass reparameterize(Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n    def call(self, inputs):\n        mean, logvar = inputs\n        batch = tf.shape(mean)[0]\n        dim = tf.shape(mean)[1]\n        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n        return eps * tf.exp(logvar * .5) + mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoder\ninput_shape = (64, 64, 3)\nkernel_size = 5\nimage_size = 64 # square, original to be resized\ncrop_size = 148 # center crop to this size per dimension\nbatch_size = 128\ngamma_init = tf.random_normal_initializer(1., 0.02)\n\n# First build the Encoder Model part\ninputs = Input(shape=input_shape, name='encoder_input')\nx = Conv2D(filters=64, kernel_size=kernel_size, strides=2, padding='same', name='conv1')(inputs)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2D(filters=128, kernel_size=kernel_size, strides=2, padding='same', name='conv2')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2D(filters=256, kernel_size=kernel_size, strides=2, padding='same', name='conv3')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2D(filters=512, kernel_size=kernel_size, strides=2, padding='same', name='conv4')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\n\nshape = K.int_shape(x)\n\nx = Flatten()(x)\nmean = Dense(latent_dim, name=\"z_mean\")(x)\nlogvar = Dense(latent_dim, name=\"z_log_var\")(x)\nz = reparameterize()([mean, logvar])\n\n# Instantiate Encoder Model\nencoder = Model(inputs, [z, mean, logvar], name='encoder')\n# encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decoder\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\nx = Dense(shape[1] * shape[2] * shape[3] * 2)(latent_inputs)\nx = Reshape((shape[1]*2, shape[2]*2, shape[3]//2), name='h0_reshape')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\n\nx = Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same')(x)\nx = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\nx = ReLU()(x)\nx = Conv2DTranspose(filters=3, kernel_size=5, strides=1, padding='same')(x)\noutputs = Activation('tanh', name='decoder_output')(x)\n\n# Instantiate Decoder Model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VAE(Model):\n    def __init__(self, encoder, latent_dim, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.cf_layer = Dense(units=latent_dim, kernel_initializer=tf.constant_initializer(np.eye(latent_dim)))\n        # self.cf_layer = Sequential(\n        #     [\n        #     Input(shape=(latent_dim,)),\n        #     Dense(32, activation=\"relu\"),\n        #     Dense(latent_dim),\n        #     ]\n        # )\n        self.decoder = decoder\n\n    def train_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n        with tf.GradientTape() as tape:\n            z, mean, logvar = encoder(data)\n            reconstruction = decoder(z)\n            reconstruction_loss = tf.reduce_mean(\n                tf.keras.losses.binary_crossentropy(data, reconstruction)\n            )\n            reconstruction_loss *= 64 * 64\n            kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n            kl_loss = tf.reduce_mean(kl_loss)\n#             kl_loss = tf.reduce_sum(kl_loss)\n            kl_loss *= -0.5\n#             LOSS_FACTOR = 10000\n            total_loss = reconstruction_loss + kl_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n    def call(self, inputs):\n      z, mean, logvar = encoder(inputs)\n      return decoder(z)\n\n    def cf(self, inputs):\n      z, mean, logvar = encoder(inputs)\n      return decoder(self.cf_layer(z))\n\n    @tf.function\n    def sample(self, eps=None):\n      if eps is None:\n        eps = tf.random.normal(shape=(100, latent_dim))\n      return tf.sigmoid(self.decoder(eps))\n\n# Instantiate vae\nvae = VAE(encoder, latent_dim, decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.07\n# vae.load_weights('../input/hyper-alpha/alpha/cgen-512-'+str(alpha)+'-100/cgen-512-'+str(alpha)+'-100/model.tf')\n# vae.load_weights('../input/celeba-alpha/cgen-512-'+str(alpha)+'-200/cgen-512-'+str(alpha)+'-200/model.tf')\n# vae.load_weights('../input/celeba-latentdim/dimension/cgen-'+str(latent_dim)+'-0.01-200/cgen-'+str(latent_dim)+'-0.01-200/model.tf')\nvae.load_weights('../input/smile-cgen-400/cgen-smile-400-0.01-200/model.tf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set variables \nmain_folder = '../input/celeba-dataset/'\nimages_folder = main_folder + 'img_align_celeba/img_align_celeba/'\nEXAMPLE_PIC = images_folder + '000506.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the data set that include the attribute for each picture\ndf_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\ndf_attr.set_index('image_id', inplace=True)\ndf_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\ndf_attr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glass = df_attr[\"Smiling\"]\n# In gender array 0-no glass while 1-glass\nglass.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_partition = pd.read_csv(main_folder + 'list_eval_partition.csv')\n# df_partition.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_partition['partition'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_partition.set_index('image_id', inplace=True)\ndf_par_attr = df_partition.join(glass, how='inner')\ndf_par_attr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ = df_par_attr[(df_par_attr['partition'] == 2) \n                           & (df_par_attr['Smiling'] == 0)][0:1]\n# .sample(1)\n# iloc[0]\ndf_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_reshape_img(fname):\n    x = cv2.imread(fname)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    x = cv2.resize(x, (64,64)).astype('float32') / 255.\n    x = x.reshape((1,) + x.shape)\n\n    return x\n\n\ndef generate_df(partition, attr, num_samples):\n    '''\n    partition\n        0 -> train\n        1 -> validation\n        2 -> test\n    \n    '''\n    \n    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n                           & (df_par_attr[attr] == 0)][0:int(num_samples/2)]\n    df_ = pd.concat([df_,\n                      df_par_attr[(df_par_attr['partition'] == partition) \n                                  & (df_par_attr[attr] == 1)][int(num_samples/2):num_samples]])\n\n    # for Train and Validation\n    if partition != 2:\n        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n        x_ = x_.reshape(x_.shape[0], 64, 64, 3)\n        y_ = np_utils.to_categorical(df_[attr],2)\n    # for Test\n    else:\n        x_ = []\n        y_ = []\n\n        for index, target in df_.iterrows():\n            im = cv2.imread(images_folder + index)\n            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n            im = np.expand_dims(im, axis =0)\n            x_.append(im)\n            y_.append(target[attr])\n\n    return x_, y_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_SAMPLES = 500\nIMG_WIDTH = 64\nIMG_HEIGHT = 64\n\nx_train, y_train = generate_df(2, 'Smiling', TRAINING_SAMPLES)\n# x_valid, y_valid = generate_df(1, 'Eyeglasses', VALIDATION_SAMPLES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 0\nfor i in range(len(x_train)):\n    if i==0:\n        temp = x_train[i]\n    else:\n        temp  = np.concatenate((temp, x_train[i]), axis=0)\nx_train = temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# plt.imshow(x_train[1501])\nplt.imshow(x_train[8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = vae.cf(x_train[7:9])\nprint(result[1].shape)\nplt.imshow(result[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_array(images):\n    if type(images) == 'tensorflow.python.framework.ops.EagerTensor':\n        images = K.eval(images)\n    for i in range(len(images)):\n        raw_0 = images[i]\n        raw_0 = np.uint8(raw_0 * 255)\n#         raw_0 = raw_0.reshape((28, 28))\n#         raw_0 = np.array(Image.fromarray(raw_0).convert(\"RGB\"))\n        raw_0 = raw_0[np.newaxis, :]\n        if i == 0:\n            raw = raw_0\n        else:\n            raw = np.concatenate((raw, raw_0), axis=0)\n    return raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ori_img = x_train[:250]\ntest_tar_img = x_train[250:]\n\nrecon_ori = vae(test_ori_img)\nrecon_ori = K.eval(recon_ori)\n\nrecon_target = vae(test_tar_img)\nrecon_target = K.eval(recon_target)\n\ncf_ori = vae.cf(test_ori_img)\ncf_ori = K.eval(cf_ori)\n\ncf_tar = vae.cf(test_tar_img)\ncf_tar = K.eval(cf_tar)\n\n\n# clf = load_model(models_dir + 'classifier_trained_on_' + str(target_class) + '.tf')\n\n#.............................\nclf_result = clf(cf_ori)\ncount = 0\nfor i in clf_result:\n  if i >= 0.5:\n    count += 1\nprint(\"clfed target percentage for cf: \", count/len(clf_result))\nprint(count, len(clf_result))\n\ncount = 0\nclf_result = clf(recon_ori)\nfor i in clf_result:\n  if i >= 0.5:\n    count += 1\nprint(\"clfed target percentage for vae: \", count/len(clf_result))\nprint(count, len(clf_result))\n\n#.............example................\nraw_0 = K.eval(cf_ori)[8]\nraw_0 = np.uint8(raw_0 * 255)\n\nplt.imshow(raw_0)\nplt.show()\n\nraw_ori = generate_array(test_ori_img)\nraw_tar = generate_array(test_tar_img)\nrecon_ori = generate_array(recon_ori)\nrecon_tar = generate_array(recon_target)\ncf_ori = generate_array(cf_ori)\ncf_tar = generate_array(cf_tar)\n\nprint(cf_ori.shape)\n\nraw_ori = raw_ori.reshape([-1,64,64,3]).transpose([0,3,1,2])\nraw_tar = raw_tar.reshape([-1,64,64,3]).transpose([0,3,1,2])\nrecon_ori = recon_ori.reshape([-1,64,64,3]).transpose([0,3,1,2])\nrecon_tar = recon_tar.reshape([-1,64,64,3]).transpose([0,3,1,2])\ncf_ori = cf_ori.reshape([-1,64,64,3]).transpose([0,3,1,2])\ncf_tar = cf_tar.reshape([-1,64,64,3]).transpose([0,3,1,2])\n\nrecon_ori.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-gan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport os\nimport functools\nimport numpy as np\nimport time\nfrom tensorflow.python.ops import array_ops\nimport tensorflow_gan as tfgan\n\nsession=tf.compat.v1.InteractiveSession()\n# A smaller BATCH_SIZE reduces GPU memory usage, but at the cost of a slight slowdown\nBATCH_SIZE = 50\n\n# Run images through Inception.\ninception_images = tf.compat.v1.placeholder(tf.float32, [None, 3, None, None], name = 'inception_images')\nactivations1 = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'activations1')\nactivations2 = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'activations2')\nfcd = tfgan.eval.frechet_classifier_distance_from_activations(activations1, activations2)\n\nINCEPTION_TFHUB = 'https://tfhub.dev/tensorflow/tfgan/eval/inception/1'\nINCEPTION_FINAL_POOL = 'pool_3'\n\ndef inception_activations(images = inception_images, num_splits = 1):\n    images = tf.transpose(images, [0, 2, 3, 1])\n    size = 299\n    images = tf.compat.v1.image.resize_bilinear(images, [size, size])\n    generated_images_list = array_ops.split(images, num_or_size_splits = num_splits)\n    activations = tf.map_fn(\n        fn = tfgan.eval.classifier_fn_from_tfhub(INCEPTION_TFHUB, INCEPTION_FINAL_POOL, True),\n        elems = array_ops.stack(generated_images_list),\n        parallel_iterations = 1,\n        back_prop = False,\n        swap_memory = True,\n        name = 'RunClassifier')\n    activations = array_ops.concat(array_ops.unstack(activations), 0)\n    return activations\n\nactivations =inception_activations()\n\ndef get_inception_activations(inps):\n    n_batches = int(np.ceil(float(inps.shape[0]) / BATCH_SIZE))\n    act = np.zeros([inps.shape[0], 2048], dtype = np.float32)\n    for i in range(n_batches):\n        inp = inps[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] / 255. * 2 - 1\n        act[i * BATCH_SIZE : i * BATCH_SIZE + min(BATCH_SIZE, inp.shape[0])] = session.run(activations, feed_dict = {inception_images: inp})\n    return act\n\ndef activations2distance(act1, act2):\n    return session.run(fcd, feed_dict = {activations1: act1, activations2: act2})\n        \ndef get_fid(images1, images2):\n    session=tf.get_default_session()\n    assert(type(images1) == np.ndarray)\n    assert(len(images1.shape) == 4)\n    assert(images1.shape[1] == 3)\n    assert(np.min(images1[0]) >= 0 and np.max(images1[0]) > 10), 'Image values should be in the range [0, 255]'\n    assert(type(images2) == np.ndarray)\n    assert(len(images2.shape) == 4)\n    assert(images2.shape[1] == 3)\n    assert(np.min(images2[0]) >= 0 and np.max(images2[0]) > 10), 'Image values should be in the range [0, 255]'\n    assert(images1.shape == images2.shape), 'The two numpy arrays must have the same shape'\n    print('Calculating FID with %i images from each distribution' % (images1.shape[0]))\n    start_time = time.time()\n    \n    act1 = get_inception_activations(images1)\n    act2 = get_inception_activations(images2)\n    fid = activations2distance(act1, act2)\n    print('FID calculation time: %f s' % (time.time() - start_time))\n    return fid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIM=28*28*3\nfid_recon_ori = get_fid(raw_ori, recon_ori)\nfid_cf_ori = get_fid(raw_ori, cf_ori)\n# fid_cf_tar = get_fid(raw_tar, cf_ori)\nprint('FID ori & recon_ori: %f'% fid_recon_ori)\nprint('FID ori & cf_ori: %f'% fid_cf_ori)\n# print('FID tar & cf_ori: %f'% fid_cf_tar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session=tf.compat.v1.InteractiveSession()\n# A smaller BATCH_SIZE reduces GPU memory usage, but at the cost of a slight slowdown\nBATCH_SIZE = 50\nINCEPTION_TFHUB = 'https://tfhub.dev/tensorflow/tfgan/eval/inception/1'\nINCEPTION_OUTPUT = 'logits'\n\n# Run images through Inception.\ninception_images = tf.compat.v1.placeholder(tf.float32, [None, 3, None, None], name = 'inception_images')\ndef inception_logits(images = inception_images, num_splits = 1):\n    images = tf.transpose(images, [0, 2, 3, 1])\n    size = 299\n    images = tf.compat.v1.image.resize_bilinear(images, [size, size])\n    generated_images_list = array_ops.split(images, num_or_size_splits = num_splits)\n    logits = tf.map_fn(\n        fn = tfgan.eval.classifier_fn_from_tfhub(INCEPTION_TFHUB, INCEPTION_OUTPUT, True),\n        elems = array_ops.stack(generated_images_list),\n        parallel_iterations = 8,\n        back_prop = False,\n        swap_memory = True,\n        name = 'RunClassifier')\n    logits = array_ops.concat(array_ops.unstack(logits), 0)\n    return logits\n\nlogits=inception_logits()\n\ndef get_inception_probs(inps):\n    session=tf.get_default_session()\n    n_batches = int(np.ceil(float(inps.shape[0]) / BATCH_SIZE))\n    preds = np.zeros([inps.shape[0], 1000], dtype = np.float32)\n    for i in range(n_batches):\n        inp = inps[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] / 255. * 2 - 1\n        preds[i * BATCH_SIZE : i * BATCH_SIZE + min(BATCH_SIZE, inp.shape[0])] = session.run(logits,{inception_images: inp})[:, :1000]\n    preds = np.exp(preds) / np.sum(np.exp(preds), 1, keepdims=True)\n    return preds\n\ndef preds2score(preds, splits=10):\n    scores = []\n    for i in range(splits):\n        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n        kl = np.mean(np.sum(kl, 1))\n        scores.append(np.exp(kl))\n    return np.mean(scores), np.std(scores)\n\ndef get_inception_score(images, splits=10):\n    assert(type(images) == np.ndarray)\n    assert(len(images.shape) == 4)\n    assert(images.shape[1] == 3)\n    assert(np.min(images[0]) >= 0 and np.max(images[0]) > 10), 'Image values should be in the range [0, 255]'\n    print('Calculating Inception Score with %i images in %i splits' % (images.shape[0], splits))\n    start_time=time.time()\n    preds = get_inception_probs(images)\n    mean, std = preds2score(preds, splits)\n    print('Inception Score calculation time: %f s' % (time.time() - start_time))\n    return mean, std  # Reference values: 11.38 for 50000 CIFAR-10 training set images, or mean=11.31, std=0.10 if in 10 splits.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IS between training set and test set\nis_cf_ori = get_inception_score(cf_ori)\nis_vae_ori = get_inception_score(recon_ori)\nprint('is_cf_ori: ', is_cf_ori)\nprint('is_vae_ori: ', is_vae_ori)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}