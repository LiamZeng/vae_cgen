{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:49.402541Z",
     "iopub.status.busy": "2020-08-10T04:07:49.401815Z",
     "iopub.status.idle": "2020-08-10T04:07:55.414873Z",
     "shell.execute_reply": "2020-08-10T04:07:55.416179Z"
    },
    "papermill": {
     "duration": 6.045473,
     "end_time": "2020-08-10T04:07:55.416449",
     "exception": false,
     "start_time": "2020-08-10T04:07:49.370976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2 \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, BatchNormalization, ReLU, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dropout, UpSampling2D\n",
    "from tensorflow.keras.layers import Lambda, Subtract, Add\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:55.468507Z",
     "iopub.status.busy": "2020-08-10T04:07:55.466275Z",
     "iopub.status.idle": "2020-08-10T04:07:55.469313Z",
     "shell.execute_reply": "2020-08-10T04:07:55.469962Z"
    },
    "papermill": {
     "duration": 0.028242,
     "end_time": "2020-08-10T04:07:55.470124",
     "exception": false,
     "start_time": "2020-08-10T04:07:55.441882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set variables \n",
    "main_folder = '../input/celeba-dataset/'\n",
    "images_folder = main_folder + 'img_align_celeba/img_align_celeba/'\n",
    "EXAMPLE_PIC = images_folder + '000506.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:55.513159Z",
     "iopub.status.busy": "2020-08-10T04:07:55.512461Z",
     "iopub.status.idle": "2020-08-10T04:07:56.312902Z",
     "shell.execute_reply": "2020-08-10T04:07:56.313463Z"
    },
    "papermill": {
     "duration": 0.828588,
     "end_time": "2020-08-10T04:07:56.313612",
     "exception": false,
     "start_time": "2020-08-10T04:07:55.485024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the data set that include the attribute for each picture\n",
    "df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\n",
    "df_attr.set_index('image_id', inplace=True)\n",
    "df_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\n",
    "df_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.350948Z",
     "iopub.status.busy": "2020-08-10T04:07:56.349568Z",
     "iopub.status.idle": "2020-08-10T04:07:56.353753Z",
     "shell.execute_reply": "2020-08-10T04:07:56.354328Z"
    },
    "papermill": {
     "duration": 0.026747,
     "end_time": "2020-08-10T04:07:56.354487",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.327740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "glass = df_attr[\"Eyeglasses\"]\n",
    "# In gender array 0-no glass while 1-glass\n",
    "glass.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.387274Z",
     "iopub.status.busy": "2020-08-10T04:07:56.386632Z",
     "iopub.status.idle": "2020-08-10T04:07:56.521054Z",
     "shell.execute_reply": "2020-08-10T04:07:56.520284Z"
    },
    "papermill": {
     "duration": 0.15266,
     "end_time": "2020-08-10T04:07:56.521192",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.368532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_partition = pd.read_csv(main_folder + 'list_eval_partition.csv')\n",
    "# df_partition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.557026Z",
     "iopub.status.busy": "2020-08-10T04:07:56.556196Z",
     "iopub.status.idle": "2020-08-10T04:07:56.571944Z",
     "shell.execute_reply": "2020-08-10T04:07:56.572536Z"
    },
    "papermill": {
     "duration": 0.03626,
     "end_time": "2020-08-10T04:07:56.572675",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.536415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_partition['partition'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.611771Z",
     "iopub.status.busy": "2020-08-10T04:07:56.610910Z",
     "iopub.status.idle": "2020-08-10T04:07:56.812284Z",
     "shell.execute_reply": "2020-08-10T04:07:56.812842Z"
    },
    "papermill": {
     "duration": 0.225645,
     "end_time": "2020-08-10T04:07:56.812991",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.587346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_partition.set_index('image_id', inplace=True)\n",
    "df_par_attr = df_partition.join(glass, how='inner')\n",
    "df_par_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.862680Z",
     "iopub.status.busy": "2020-08-10T04:07:56.861018Z",
     "iopub.status.idle": "2020-08-10T04:07:56.863857Z",
     "shell.execute_reply": "2020-08-10T04:07:56.864402Z"
    },
    "papermill": {
     "duration": 0.035318,
     "end_time": "2020-08-10T04:07:56.864529",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.829211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_reshape_img(fname):\n",
    "    x = cv2.imread(fname)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = cv2.resize(x, (64,64)).astype('float32') / 255.\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_df(partition, attr, num_samples):\n",
    "    '''\n",
    "    partition\n",
    "        0 -> train\n",
    "        1 -> validation\n",
    "        2 -> test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n",
    "                           & (df_par_attr[attr] == 0)].sample(int(num_samples/2))\n",
    "    df_ = pd.concat([df_,\n",
    "                      df_par_attr[(df_par_attr['partition'] == partition) \n",
    "                                  & (df_par_attr[attr] == 1)].sample(int(num_samples/2))])\n",
    "\n",
    "    # for Train and Validation\n",
    "    if partition != 2:\n",
    "        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n",
    "        x_ = x_.reshape(x_.shape[0], 64, 64, 3)\n",
    "        y_ = np_utils.to_categorical(df_[attr],2)\n",
    "    # for Test\n",
    "    else:\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        for index, target in df_.iterrows():\n",
    "            im = cv2.imread(images_folder + index)\n",
    "            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n",
    "            im = np.expand_dims(im, axis =0)\n",
    "            x_.append(im)\n",
    "            y_.append(target[attr])\n",
    "\n",
    "    return x_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:07:56.902078Z",
     "iopub.status.busy": "2020-08-10T04:07:56.900908Z",
     "iopub.status.idle": "2020-08-10T04:08:14.153753Z",
     "shell.execute_reply": "2020-08-10T04:08:14.152642Z"
    },
    "papermill": {
     "duration": 17.273414,
     "end_time": "2020-08-10T04:08:14.153888",
     "exception": false,
     "start_time": "2020-08-10T04:07:56.880474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = 50000\n",
    "VALIDATION_SAMPLES = 500\n",
    "IMG_WIDTH = 64\n",
    "IMG_HEIGHT = 64\n",
    "\n",
    "x_train, y_train = generate_df(0, 'Eyeglasses', TRAINING_SAMPLES)\n",
    "# x_valid, y_valid = generate_df(1, 'Eyeglasses', VALIDATION_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:14.192411Z",
     "iopub.status.busy": "2020-08-10T04:08:14.191559Z",
     "iopub.status.idle": "2020-08-10T04:08:14.235313Z",
     "shell.execute_reply": "2020-08-10T04:08:14.235823Z"
    },
    "papermill": {
     "duration": 0.065972,
     "end_time": "2020-08-10T04:08:14.235956",
     "exception": false,
     "start_time": "2020-08-10T04:08:14.169984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(5000):\n",
    "    if (y_train[i]==np.array([1,0])).all():\n",
    "        count += 1\n",
    "print(x_train.shape)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016077,
     "end_time": "2020-08-10T04:08:14.268308",
     "exception": false,
     "start_time": "2020-08-10T04:08:14.252231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CLF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:14.313682Z",
     "iopub.status.busy": "2020-08-10T04:08:14.313068Z",
     "iopub.status.idle": "2020-08-10T04:08:17.989223Z",
     "shell.execute_reply": "2020-08-10T04:08:17.988239Z"
    },
    "papermill": {
     "duration": 3.70493,
     "end_time": "2020-08-10T04:08:17.989398",
     "exception": false,
     "start_time": "2020-08-10T04:08:14.284468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "kernel_size = 5\n",
    "gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "\n",
    "\n",
    "class glass_clf(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "      super(glass_clf, self).__init__(**kwargs)\n",
    "      self.f2l = Sequential(\n",
    "          [\n",
    "           Input(shape=input_shape, name='classifier_input'),\n",
    "           Conv2D(filters=64, kernel_size=kernel_size, strides=2, padding='same'),\n",
    "           BatchNormalization(gamma_initializer=gamma_init, trainable=True),\n",
    "           ReLU(),\n",
    "           Conv2D(filters=128, kernel_size=kernel_size, strides=2, padding='same'),\n",
    "           BatchNormalization(gamma_initializer=gamma_init, trainable=True),\n",
    "           ReLU(),\n",
    "           Conv2D(filters=256, kernel_size=kernel_size, strides=2, padding='same'),\n",
    "           BatchNormalization(gamma_initializer=gamma_init, trainable=True),\n",
    "           ReLU(),\n",
    "           Flatten(),\n",
    "           Dense(512, activation='relu'),\n",
    "          ]\n",
    "      )\n",
    "      self.last = Sequential(\n",
    "          [\n",
    "           Input(shape=(512,)),\n",
    "#            Dense(units=2, activation = 'softmax'),\n",
    "           Dense(units=1, activation = 'sigmoid'),\n",
    "          ]\n",
    "      )\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "      return self.last(self.f2l(inputs))\n",
    "\n",
    "clf = glass_clf()\n",
    "clf.build(input_shape=(None, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015725,
     "end_time": "2020-08-10T04:08:18.021712",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.005987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:18.062765Z",
     "iopub.status.busy": "2020-08-10T04:08:18.062009Z",
     "iopub.status.idle": "2020-08-10T04:08:18.065471Z",
     "shell.execute_reply": "2020-08-10T04:08:18.066003Z"
    },
    "papermill": {
     "duration": 0.027987,
     "end_time": "2020-08-10T04:08:18.066140",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.038153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a sampling layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "class reparameterize(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return eps * tf.exp(logvar * .5) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:18.116743Z",
     "iopub.status.busy": "2020-08-10T04:08:18.109879Z",
     "iopub.status.idle": "2020-08-10T04:08:18.481211Z",
     "shell.execute_reply": "2020-08-10T04:08:18.482051Z"
    },
    "papermill": {
     "duration": 0.400677,
     "end_time": "2020-08-10T04:08:18.482265",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.081588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "input_shape = (64, 64, 3)\n",
    "kernel_size = 5\n",
    "latent_dim = 512\n",
    "# latent_dim = 256\n",
    "image_size = 64 # square, original to be resized\n",
    "crop_size = 148 # center crop to this size per dimension\n",
    "batch_size = 128\n",
    "gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "\n",
    "# First build the Encoder Model part\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(filters=64, kernel_size=kernel_size, strides=2, padding='same', name='conv1')(inputs)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2D(filters=128, kernel_size=kernel_size, strides=2, padding='same', name='conv2')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2D(filters=256, kernel_size=kernel_size, strides=2, padding='same', name='conv3')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2D(filters=512, kernel_size=kernel_size, strides=2, padding='same', name='conv4')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "logvar = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = reparameterize()([mean, logvar])\n",
    "\n",
    "# Instantiate Encoder Model\n",
    "encoder = Model(inputs, [z, mean, logvar], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:18.533555Z",
     "iopub.status.busy": "2020-08-10T04:08:18.530884Z",
     "iopub.status.idle": "2020-08-10T04:08:18.806613Z",
     "shell.execute_reply": "2020-08-10T04:08:18.806076Z"
    },
    "papermill": {
     "duration": 0.306821,
     "end_time": "2020-08-10T04:08:18.806743",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.499922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1] * shape[2] * shape[3] * 2)(latent_inputs)\n",
    "x = Reshape((shape[1]*2, shape[2]*2, shape[3]//2), name='h0_reshape')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same')(x)\n",
    "x = BatchNormalization(gamma_initializer=gamma_init, trainable=True)(x)\n",
    "x = ReLU()(x)\n",
    "x = Conv2DTranspose(filters=3, kernel_size=5, strides=1, padding='same')(x)\n",
    "outputs = Activation('tanh', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:18.865822Z",
     "iopub.status.busy": "2020-08-10T04:08:18.862524Z",
     "iopub.status.idle": "2020-08-10T04:08:18.868758Z",
     "shell.execute_reply": "2020-08-10T04:08:18.868238Z"
    },
    "papermill": {
     "duration": 0.044438,
     "end_time": "2020-08-10T04:08:18.868888",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.824450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, latent_dim, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.cf_layer = Dense(units=latent_dim, kernel_initializer=tf.constant_initializer(np.eye(latent_dim)))\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z, mean, logvar = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 64 * 64\n",
    "            kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "#             kl_loss = tf.reduce_sum(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "#             LOSS_FACTOR = 10000\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "      z, mean, logvar = encoder(inputs)\n",
    "      return decoder(z)\n",
    "\n",
    "    def cf(self, inputs):\n",
    "      z, mean, logvar = encoder(inputs)\n",
    "      return decoder(self.cf_layer(z))\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "      if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "      return tf.sigmoid(self.decoder(eps))\n",
    "\n",
    "# Instantiate vae\n",
    "vae = VAE(encoder, latent_dim, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:18.937816Z",
     "iopub.status.busy": "2020-08-10T04:08:18.932747Z",
     "iopub.status.idle": "2020-08-10T04:08:19.283564Z",
     "shell.execute_reply": "2020-08-10T04:08:19.284340Z"
    },
    "papermill": {
     "duration": 0.398519,
     "end_time": "2020-08-10T04:08:19.284515",
     "exception": false,
     "start_time": "2020-08-10T04:08:18.885996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pretrained weights\n",
    "vae.load_weights('../input/cgen-pretrain-vae/CELEBVAE_45.tf')\n",
    "# vae.load_weights('../input/cage-pretrain-vae-256/CELEBVAE_20_256.tf')\n",
    "clf.load_weights('../input/glassclf-sigmoid/glass_cf_15_sigmoid.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017507,
     "end_time": "2020-08-10T04:08:19.319002",
     "exception": false,
     "start_time": "2020-08-10T04:08:19.301495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **simple test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:19.357622Z",
     "iopub.status.busy": "2020-08-10T04:08:19.356688Z",
     "iopub.status.idle": "2020-08-10T04:08:19.574199Z",
     "shell.execute_reply": "2020-08-10T04:08:19.573359Z"
    },
    "papermill": {
     "duration": 0.238678,
     "end_time": "2020-08-10T04:08:19.574357",
     "exception": false,
     "start_time": "2020-08-10T04:08:19.335679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple test\n",
    "test_img = load_reshape_img('../input/celeba-dataset/img_align_celeba/img_align_celeba/000144.jpg')\n",
    "plt.imshow(test_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:19.618086Z",
     "iopub.status.busy": "2020-08-10T04:08:19.617512Z",
     "iopub.status.idle": "2020-08-10T04:08:23.215800Z",
     "shell.execute_reply": "2020-08-10T04:08:23.215153Z"
    },
    "papermill": {
     "duration": 3.622529,
     "end_time": "2020-08-10T04:08:23.215926",
     "exception": false,
     "start_time": "2020-08-10T04:08:19.593397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_result = vae(test_img)\n",
    "test_clf = clf(test_img)\n",
    "print(test_clf)\n",
    "plt.imshow(test_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018384,
     "end_time": "2020-08-10T04:08:23.252936",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.234552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **train cgen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:23.296022Z",
     "iopub.status.busy": "2020-08-10T04:08:23.295195Z",
     "iopub.status.idle": "2020-08-10T04:08:23.299315Z",
     "shell.execute_reply": "2020-08-10T04:08:23.298772Z"
    },
    "papermill": {
     "duration": 0.026954,
     "end_time": "2020-08-10T04:08:23.299464",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.272510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = y_train\n",
    "# test_result = [np.argmax(one_hot)for one_hot in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:23.341725Z",
     "iopub.status.busy": "2020-08-10T04:08:23.339656Z",
     "iopub.status.idle": "2020-08-10T04:08:23.342622Z",
     "shell.execute_reply": "2020-08-10T04:08:23.343127Z"
    },
    "papermill": {
     "duration": 0.025769,
     "end_time": "2020-08-10T04:08:23.343250",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.317481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(x_train[6500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:23.398749Z",
     "iopub.status.busy": "2020-08-10T04:08:23.388320Z",
     "iopub.status.idle": "2020-08-10T04:08:23.643212Z",
     "shell.execute_reply": "2020-08-10T04:08:23.642575Z"
    },
    "papermill": {
     "duration": 0.281858,
     "end_time": "2020-08-10T04:08:23.643353",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.361495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "y_interger = [np.argmax(one_hot)for one_hot in y_train]\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_interger))\n",
    "                     .shuffle(len(x_train)).batch(batch_size))\n",
    "\n",
    "def step(vae, classifier, optimizer, optimizer2, batch_size, alpha):\n",
    "    vae.cf_layer.trainable = True\n",
    "    total = 0\n",
    "    vae_loss = 0\n",
    "    clf_loss = 0\n",
    "    disc_total = 0\n",
    "    batch_num = 0\n",
    "    for batch in tqdm(train_dataset):\n",
    "        with tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "            batch_num += 1\n",
    "            counterfactual = vae.cf(batch[0])\n",
    "            # for d_g\n",
    "            reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(batch[0], counterfactual))\n",
    "            d_g = reconstruction_loss\n",
    "\n",
    "            index_ori = np.where(K.eval(batch[1]) == 0)\n",
    "            index_target = np.where(K.eval(batch[1]) == 1)\n",
    "            target_tensor = np.ones([len(batch[0]), 1])\n",
    "            target_tensor[index_ori[0]] = target_tensor[index_ori[0]]* 0.1\n",
    "            target_tensor[index_target[0]] = target_tensor[index_target[0]]* 0.9\n",
    "\n",
    "            # for d_c\n",
    "            logits = classifier(counterfactual)\n",
    "            disc_loss = tf.reduce_mean(tf.reduce_mean(tf.keras.losses.binary_crossentropy(target_tensor, logits)))\n",
    "            target_tensor = np.ones([len(batch[0]), 1])\n",
    "            target_tensor[index_ori[0]] = target_tensor[index_ori[0]]* 0.9\n",
    "            target_tensor = tf.convert_to_tensor(target_tensor, dtype=tf.float32)\n",
    "            d_c = tf.reduce_mean(tf.keras.losses.binary_crossentropy(target_tensor, logits))\n",
    "            total_loss = (1. - alpha)*d_g + alpha * d_c\n",
    "            total += total_loss\n",
    "            vae_loss += d_g\n",
    "            clf_loss += d_c\n",
    "            disc_total += disc_loss\n",
    "            # clf_loss += d_c*len(batch)\n",
    "        dec_gradients = dec_tape.gradient(total_loss, vae.cf_layer.trainable_weights)\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, classifier.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(dec_gradients, vae.cf_layer.trainable_weights))\n",
    "        optimizer2.apply_gradients(zip(disc_gradients, classifier.trainable_weights))\n",
    "    return [total/batch_num, vae_loss/batch_num, clf_loss/batch_num, disc_total/batch_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:23.738121Z",
     "iopub.status.busy": "2020-08-10T04:08:23.737500Z",
     "iopub.status.idle": "2020-08-10T04:08:23.755892Z",
     "shell.execute_reply": "2020-08-10T04:08:23.755335Z"
    },
    "papermill": {
     "duration": 0.042687,
     "end_time": "2020-08-10T04:08:23.756012",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.713325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_img1 = load_reshape_img('../input/celeba-dataset/img_align_celeba/img_align_celeba/199998.jpg')\n",
    "test_img2 = load_reshape_img('../input/celeba-dataset/img_align_celeba/img_align_celeba/201557.jpg')\n",
    "test_img3 = load_reshape_img('../input/celeba-dataset/img_align_celeba/img_align_celeba/200003.jpg')\n",
    "test_img4 = load_reshape_img('../input/celeba-dataset/img_align_celeba/img_align_celeba/200233.jpg')\n",
    "\n",
    "test_img1 = np.concatenate([test_img1, test_img2])\n",
    "test_img1 = np.concatenate([test_img1, test_img3])\n",
    "test_img = np.concatenate([test_img1, test_img4])\n",
    "\n",
    "\n",
    "\n",
    "# valid_dataset = (tf.data.Dataset.from_tensor_slices(x_valid).batch(batch_size))\n",
    "# for test_batch in valid_dataset.take(1):\n",
    "#     test_img = test_batch[0:4, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:23.803824Z",
     "iopub.status.busy": "2020-08-10T04:08:23.802830Z",
     "iopub.status.idle": "2020-08-10T04:08:24.090564Z",
     "shell.execute_reply": "2020-08-10T04:08:24.091054Z"
    },
    "papermill": {
     "duration": 0.317006,
     "end_time": "2020-08-10T04:08:24.091216",
     "exception": false,
     "start_time": "2020-08-10T04:08:23.774210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_save_cf(model, test_sample):\n",
    "    result = model.cf(test_sample)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(result, test_sample))\n",
    "    print(reconstruction_loss)\n",
    "    for i in range(result.shape[0]):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(result[i, :, :, :])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_and_save_cf(vae, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:08:24.146074Z",
     "iopub.status.busy": "2020-08-10T04:08:24.145090Z",
     "iopub.status.idle": "2020-08-10T04:43:44.905206Z",
     "shell.execute_reply": "2020-08-10T04:43:44.905748Z"
    },
    "papermill": {
     "duration": 2120.795031,
     "end_time": "2020-08-10T04:43:44.905918",
     "exception": false,
     "start_time": "2020-08-10T04:08:24.110887",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_1 = 0.5\n",
    "vae = VAE(encoder, latent_dim, decoder)\n",
    "vae.load_weights('../input/cgen-pretrain-vae/CELEBVAE_45.tf')\n",
    "clf.load_weights('../input/glassclf-sigmoid/glass_cf_15_sigmoid.tf')\n",
    "\n",
    "optimizer=Adam(beta_1=beta_1)\n",
    "optimizer2 = Adam()\n",
    "batch_size = 128\n",
    "total_loss = []\n",
    "vae_loss = []\n",
    "clf_loss = []\n",
    "dis_loss = []\n",
    "\n",
    "train_cycles = 200\n",
    "for train_cycle in range(train_cycles):\n",
    "    print(\"Cgen train cycle:\", train_cycle, train_cycles)\n",
    "    # cGen training\n",
    "    cgen_loss = step(vae, clf, optimizer, optimizer2, batch_size, 0.01)\n",
    "    print('cgen_loss: {a}, vae_loss: {b}, clf_loss: {c}, disc_loss: {d}'.format(a=cgen_loss[0], b=cgen_loss[1], c=cgen_loss[2], d=cgen_loss[-1]))\n",
    " \n",
    "    generate_and_save_cf(vae, test_img)\n",
    "    a = K.eval(cgen_loss[0])\n",
    "    b = K.eval(cgen_loss[1])\n",
    "    c = K.eval(cgen_loss[2])\n",
    "    d = K.eval(cgen_loss[3])\n",
    "    total_loss.append(a)\n",
    "    vae_loss.append(b)\n",
    "    clf_loss.append(c)\n",
    "    dis_loss.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:43:46.471882Z",
     "iopub.status.busy": "2020-08-10T04:43:46.470998Z",
     "iopub.status.idle": "2020-08-10T04:43:46.720461Z",
     "shell.execute_reply": "2020-08-10T04:43:46.721052Z"
    },
    "papermill": {
     "duration": 1.051776,
     "end_time": "2020-08-10T04:43:46.721209",
     "exception": false,
     "start_time": "2020-08-10T04:43:45.669433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "ax_1.plot(np.arange(1, len(total_loss)) * 1, total_loss[1:], label='total')\n",
    "ax_1.plot(np.arange(1, len(vae_loss)) * 1, vae_loss[1:], label='first term')\n",
    "# ax_1.plot(np.arange(1, len(clf_loss)) * 1, clf_loss[1:], label='second term')\n",
    "# ax_1.plot(np.arange(1, len(dis_loss)) * 1, dis_loss[1:], label='disc term')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "ax_1.set_ylabel('binary cross entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:43:48.474465Z",
     "iopub.status.busy": "2020-08-10T04:43:48.472925Z",
     "iopub.status.idle": "2020-08-10T04:43:48.477288Z",
     "shell.execute_reply": "2020-08-10T04:43:48.477809Z"
    },
    "papermill": {
     "duration": 0.984769,
     "end_time": "2020-08-10T04:43:48.477950",
     "exception": false,
     "start_time": "2020-08-10T04:43:47.493181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_loss = np.array(total_loss)\n",
    "vae_loss = np.array(vae_loss)\n",
    "clf_loss = np.array(clf_loss)\n",
    "\n",
    "np.save('total.npy',total_loss)\n",
    "np.save('first.npy', vae_loss)\n",
    "np.save('sec.npy',clf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-10T04:43:49.957168Z",
     "iopub.status.busy": "2020-08-10T04:43:49.956304Z",
     "iopub.status.idle": "2020-08-10T04:43:50.128192Z",
     "shell.execute_reply": "2020-08-10T04:43:50.127045Z"
    },
    "papermill": {
     "duration": 0.9354,
     "end_time": "2020-08-10T04:43:50.128320",
     "exception": false,
     "start_time": "2020-08-10T04:43:49.192920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae.save_weights('cgen-512-0.01-200/model.tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "papermill": {
   "duration": 2167.686344,
   "end_time": "2020-08-10T04:43:52.842876",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-10T04:07:45.156532",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
