{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "To5W_dRRhXN6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Lambda, Subtract, Add\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import datetime\n",
    "!/opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eABRscUiiX0O"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoCHqVBwid-s"
   },
   "outputs": [],
   "source": [
    "%cd \"/content/drive/My Drive/cgen/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDB_XsM7iR87"
   },
   "outputs": [],
   "source": [
    "original_class = 0\n",
    "target_class = 8\n",
    "alpha = 0.1\n",
    "cgen_learning_rate =  0.005\n",
    "beta_1 = 0.5\n",
    "classifier_def = {'true_label' : 0.9,\n",
    "                 'false_label' : 0.1  # label smoothing\n",
    "                 }\n",
    "\n",
    "models_dir = './models/'\n",
    "if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "models_best_dir = './models/best/'\n",
    "if not os.path.exists(models_best_dir):\n",
    "            os.makedirs(models_best_dir)\n",
    "imgs_save_dir = models_dir + 'results/'\n",
    "if not os.path.exists(imgs_save_dir):\n",
    "            os.makedirs(imgs_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hicuCMcaiktI"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1Uq4YJHioul"
   },
   "outputs": [],
   "source": [
    "# Getting sub set from Mnist with only zeros\n",
    "index_original_class = np.where(y_train == original_class)\n",
    "x_train_original_class = x_train[index_original_class[0]]\n",
    "index_test_original_class = np.where(y_test == original_class)\n",
    "x_test_original_class = x_test[index_test_original_class[0]]\n",
    "index_test_target_class = np.where(y_test == target_class)\n",
    "x_test_target_class = x_test[index_test_target_class[0]]\n",
    "index_target_class = np.where(y_train == target_class)\n",
    "x_train_target_class = x_train[index_target_class[0]]\n",
    "\n",
    "x_train_original_class[1].shape\n",
    "plt.imshow(x_train_original_class[13], cmap='gray')\n",
    "plt.show()\n",
    "print(x_test_target_class[10].shape)\n",
    "plt.imshow(x_test_target_class[10], cmap='gray')\n",
    "plt.show()\n",
    "# reshape\n",
    "image_size = x_train_original_class.shape[1]\n",
    "x_train_original_class = np.reshape(x_train_original_class, [-1, image_size, image_size, 1])\n",
    "x_train_original_class = x_train_original_class.astype('float32') / 255\n",
    "x_train_target_class = np.reshape(x_train_target_class, [-1, image_size, image_size, 1])\n",
    "x_train_target_class = x_train_target_class.astype('float32') / 255\n",
    "x_test_original_class = np.reshape(x_test_original_class, [-1, image_size, image_size, 1])\n",
    "x_test_original_class = x_test_original_class.astype('float32') / 255\n",
    "x_test_target_class = np.reshape(x_test_target_class, [-1, image_size, image_size, 1])\n",
    "x_test_target_class = x_test_target_class.astype('float32') / 255\n",
    "x_train_classifier = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_train_classifier = x_train_classifier.astype('float32') / 255\n",
    "x_test_classifier = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
    "x_test_classifier = x_test_classifier.astype('float32') / 255\n",
    "print(x_train.shape)\n",
    "print(x_train_original_class.shape)\n",
    "print(x_train_target_class.shape)\n",
    "\n",
    "y_train_classifier = np.zeros(y_train.shape)\n",
    "y_index_target_class = np.where(y_train == target_class)\n",
    "y_train_classifier[y_index_target_class[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MkwV8lkiorc"
   },
   "outputs": [],
   "source": [
    "# create a sampling layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "class reparameterize(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return eps * tf.exp(logvar * .5) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HDpNBq2iooz"
   },
   "outputs": [],
   "source": [
    "# Generator: Encoder + Decoder (VAE)\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 10\n",
    "\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "x = Conv2D(filters=32, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x)\n",
    "x = Conv2D(filters=64, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "logvar = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = reparameterize()([mean, logvar])\n",
    "\n",
    "# Instantiate Encoder Model\n",
    "encoder = Model(inputs, [z, mean, logvar], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khPhKEDNiomE"
   },
   "outputs": [],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(units=7*7*32, activation=tf.nn.relu)(latent_inputs)\n",
    "x = Reshape((7, 7, 32))(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')(x)\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yLJIvqViojO"
   },
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, latent_dim, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.cf_layer = Dense(units=latent_dim, kernel_initializer=tf.constant_initializer(np.eye(latent_dim)))\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z, mean, logvar = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 28 * 28\n",
    "            kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "      z, mean, logvar = encoder(inputs)\n",
    "      return decoder(z)\n",
    "\n",
    "    def cf(self, inputs):\n",
    "      z, mean, logvar = encoder(inputs)\n",
    "      return decoder(self.cf_layer(z))\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "      if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "      return tf.sigmoid(self.decoder(eps))\n",
    "\n",
    "vae = VAE(encoder, latent_dim, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HefZwOqDjJcQ"
   },
   "outputs": [],
   "source": [
    "vae.compile(optimizer=Adam())\n",
    "\n",
    "x_vae_train = np.concatenate([x_train_original_class, x_train_target_class])\n",
    "x_vae_test = np.concatenate([x_test_original_class, x_test_target_class])\n",
    "\n",
    "vae.fit(x_vae_train,\n",
    "        x_vae_train,\n",
    "        validation_data=(x_vae_test, x_vae_test),\n",
    "        epochs=30,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "# vae.fit(x_train_original_class,\n",
    "#         x_train_original_class,\n",
    "#         validation_data=(x_test_original_class, x_test_original_class),\n",
    "#         epochs=30,\n",
    "#         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2rAf5HnjNZn"
   },
   "outputs": [],
   "source": [
    "# vae.save_weights(models_dir + \"vae/\" + 'vae_13_trained_on_' + str(original_class) + str(target_class) + '.tf')\n",
    "vae.load_weights(models_dir + \"vae/\" + 'vae_trained_on_' + str(original_class) + str(target_class) + '.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9G_OBVk-jilO"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, test_sample):\n",
    "    result = model.call(test_sample)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(result.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(result[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig('vae_MNIST_improved.pdf')\n",
    "    plt.show()\n",
    "\n",
    "x_vae_train = np.concatenate([x_train_original_class, x_train_target_class])\n",
    "x_vae_test = np.concatenate([x_test_original_class, x_test_target_class])\n",
    "test_img = []\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test_original_class).batch(batch_size))\n",
    "\n",
    "for test_batch in test_dataset.take(1):\n",
    "    test_img = test_batch[0:16, :, :, :]\n",
    "generate_and_save_images(vae, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XG_wfbq8S6Uv"
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 5\n",
    "layer_filters = [32, 64]\n",
    "\n",
    "inputs = Input(shape=input_shape, name='classifier_input')\n",
    "x = inputs\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(1, name='classifier_output', activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "# Instantiate Encoder Model\n",
    "# classifier = Model(inputs, [x, ll], name='classifier')\n",
    "classifier = Model(inputs, x, name='classifier')\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_WXOXphS-pZ"
   },
   "outputs": [],
   "source": [
    "classifier.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "classifier.fit(x_train_classifier, y_train_classifier, epochs=6, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YruM8AXD3PTZ"
   },
   "outputs": [],
   "source": [
    "classifier.save(models_dir + 'classifier_trained_on_' + str(target_class) + '.tf')\n",
    "# classifier = load_model(models_dir + 'classifier_trained_on_' + str(target_class) + '.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmLZgTZWnWua"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "input_label = np.zeros(shape=(len(x_train_original_class),))\n",
    "target_label = np.ones(shape=(len(x_train_target_class),))\n",
    "train_data = np.concatenate([x_train_original_class, x_train_target_class])\n",
    "train_label = np.concatenate([input_label, target_label])\n",
    "\n",
    "train_dataset_cgen = (tf.data.Dataset.from_tensor_slices((x_train_original_class, input_label))\n",
    "                     .shuffle(len(x_train_original_class)).batch(batch_size))\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
    "                     .shuffle(len(train_data)).batch(batch_size))\n",
    "\n",
    "def step(vae, classifier, optimizer, optimizer2, batch_size, alpha):\n",
    "    vae.cf_layer.trainable = True\n",
    "    total = 0\n",
    "    vae_loss = 0\n",
    "    clf_loss = 0\n",
    "    disc = 0\n",
    "    batch_num = 0\n",
    "    for batch in tqdm(train_dataset_cgen):\n",
    "        batch_num += 1\n",
    "        with tf.GradientTape() as dec_tape:\n",
    "            index_ori = np.where(K.eval(batch[1]) == 0)[0]\n",
    "            # index_target = np.where(K.eval(batch[1]) == 1)\n",
    "\n",
    "            train_img = tf.gather(batch[0], index_ori)\n",
    "            counterfactual = vae.cf(train_img)\n",
    "            # for d_g\n",
    "            d_g = tf.reduce_mean(tf.keras.losses.binary_crossentropy(train_img, counterfactual))\n",
    "            # for d_c\n",
    "            logits = classifier(counterfactual)\n",
    "            target_tensor = np.ones([len(train_img), 1])* 0.9\n",
    "            # target_tensor[index_ori[0]] = target_tensor[index_ori[0]]* 0.9\n",
    "            target_tensor = tf.convert_to_tensor(target_tensor, dtype=tf.float32)\n",
    "            d_c = tf.reduce_mean(tf.keras.losses.binary_crossentropy(target_tensor, logits))\n",
    "\n",
    "            total_loss = (1. - alpha)*d_g + alpha * d_c\n",
    "            total += total_loss\n",
    "            vae_loss += d_g\n",
    "            clf_loss += d_c\n",
    "\n",
    "            dec_gradients = dec_tape.gradient(total_loss, vae.cf_layer.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(dec_gradients, vae.cf_layer.trainable_weights))\n",
    "\n",
    "    for batch in tqdm(train_dataset):\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            logits = classifier(batch[0])\n",
    "            target_tensor = np.ones([len(batch[0]), 1])\n",
    "            index_ori = np.where(K.eval(batch[1]) == 0)\n",
    "            index_target = np.where(K.eval(batch[1]) == 1)\n",
    "            target_tensor[index_ori[0]] = target_tensor[index_ori[0]]* 0.1\n",
    "            target_tensor[index_target[0]] = target_tensor[index_target[0]]* 0.9\n",
    "            target_tensor = tf.convert_to_tensor(target_tensor, dtype=tf.float32)\n",
    "\n",
    "            disc_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(target_tensor, logits))\n",
    "            disc += disc_loss\n",
    "\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, classifier.trainable_weights)\n",
    "        optimizer2.apply_gradients(zip(disc_gradients, classifier.trainable_weights))\n",
    "    return [total/batch_num, vae_loss/batch_num, clf_loss/batch_num, disc/batch_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw4wjjntrD4K"
   },
   "outputs": [],
   "source": [
    "latent_dim = 13\n",
    "vae = VAE(encoder, latent_dim, decoder)\n",
    "vae.load_weights(models_dir + 'vae/' + 'vae_08_trained_on_' + str(original_class) + str(target_class) + '.tf')\n",
    "classifier = load_model(models_dir + 'classifier_trained_on_' + str(target_class) + '.tf')\n",
    "optimizer=Adam(lr=0.005, beta_1=beta_1)\n",
    "optimizer2=Adam(lr=0.005)\n",
    "batch_size = 128\n",
    "total_loss = []\n",
    "vae_loss = []\n",
    "clf_loss = []\n",
    "dis_loss = []\n",
    "\n",
    "train_cycles = 100\n",
    "for train_cycle in range(train_cycles):\n",
    "    print(\"Cgen train cycle:\", train_cycle, train_cycles)\n",
    "    # cGen training\n",
    "    cgen_loss = step(vae, classifier, optimizer, optimizer2, batch_size, 0.1)\n",
    "    print('cgen_loss: {a}, vae_loss: {b}, clf_loss: {c}, disc_loss: {d}'.format(a=cgen_loss[0], b=cgen_loss[1], c=cgen_loss[2], d=cgen_loss[-1]))\n",
    " \n",
    "    generate_and_save_cf(vae, test_img)\n",
    "    a = K.eval(cgen_loss[0])\n",
    "    b = K.eval(cgen_loss[1])\n",
    "    c = K.eval(cgen_loss[2])\n",
    "    d = K.eval(cgen_loss[3])\n",
    "    total_loss.append(a)\n",
    "    vae_loss.append(b)\n",
    "    clf_loss.append(c)\n",
    "    dis_loss.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAB9F8zmrSmf"
   },
   "outputs": [],
   "source": [
    "#  vae.load_weights('cgen_08_0.9_10/'+'epoch_{}'.format(100))\n",
    "\n",
    "def generate_and_save_cf(model, test_sample):\n",
    "    result = model.cf(test_sample)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(result, test_sample))\n",
    "    print(reconstruction_loss)\n",
    "    for i in range(result.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(result[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    # plt.savefig(\"cgen_13_0.1_13.pdf\")\n",
    "    plt.show()\n",
    "    print(classifier(result))\n",
    "\n",
    "generate_and_save_cf(vae, test_img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main_exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
